{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis using pre-trained GLove Embeddings\n",
    "\n",
    "### Glove Embeddings are the vector representation of the words trained on global word-word co-occurrence statistics from a corpus that basically count how frequently a word appears in a context.\n",
    "\n",
    "### Glove embeddings are generally preferred over word2vec on a larger dataset as they basically work on the principle of dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import nltk\n",
    "from keras.preprocessing import sequence,text\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Embedding,LSTM\n",
    "from keras.utils import to_categorical\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.tsv\", delimiter = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(156060, 4)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the data so that it becomes easier to train\n",
    "\n",
    "### stem function of nltk filters the tense of the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156060\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "c = df['SentenceId'].unique()\n",
    "n = 0\n",
    "for i in df['SentenceId'] : \n",
    "    review = re.sub('[^a-zA-Z]',' ',df['Phrase'][n]) \n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    ps = PorterStemmer()\n",
    "    review = [ps.stem(word) for word in review]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n",
    "    n = n+1\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>clean_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "      <td>a seri of escapad demonstr the adag that what ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "      <td>a seri of escapad demonstr the adag that what ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "      <td>a seri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "      <td>seri</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment                                       clean_review  \n",
       "0          1  a seri of escapad demonstr the adag that what ...  \n",
       "1          2  a seri of escapad demonstr the adag that what ...  \n",
       "2          2                                             a seri  \n",
       "3          2                                                  a  \n",
       "4          2                                               seri  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['clean_review']=corpus\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(156060, 5)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text=df.clean_review.values\n",
    "t = df.Sentiment.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(156060,) (156060, 5)\n"
     ]
    }
   ],
   "source": [
    "y=to_categorical(t)\n",
    "print(train_text.shape,y.shape)\n",
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_text,x_val_text,y_train,y_val=train_test_split(train_text,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124848,) (124848, 5)\n",
      "(31212,) (31212, 5)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_text.shape,y_train.shape)\n",
    "print(x_val_text.shape,y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['not', 'count', 'a', 'few', 'gross', 'out', 'comedi', 'i', 've', 'been', 'tri', 'to', 'forget']\n"
     ]
    }
   ],
   "source": [
    "r_len=[]\n",
    "for text in x_train_text:\n",
    "    word=word_tokenize(text)\n",
    "    l=len(word)\n",
    "    r_len.append(l)\n",
    "    \n",
    "max_len=np.max(r_len)\n",
    "max_len\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "all_words=' '.join(x_train_text)\n",
    "all_words=word_tokenize(all_words)\n",
    "dist=FreqDist(all_words)\n",
    "num_unique_word=len(dist)\n",
    "num_unique_word\n",
    "\n",
    "max_features = num_unique_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "children s song\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(x_train_text))\n",
    "print(x_train_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[377, 7, 827]\n"
     ]
    }
   ],
   "source": [
    "X_train = tokenizer.texts_to_sequences(x_train_text)\n",
    "X_val = tokenizer.texts_to_sequences(x_val_text)\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124848, 48) (31212, 48)\n"
     ]
    }
   ],
   "source": [
    "X_train = sequence.pad_sequences(X_train, maxlen=max_len)\n",
    "X_val = sequence.pad_sequences(X_val, maxlen=max_len)\n",
    "print(X_train.shape,X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n"
     ]
    }
   ],
   "source": [
    "file =  \"glove.6B.100d.txt\"\n",
    "f = open(file , encoding=\"utf8\")\n",
    "embedd_index = {}\n",
    "for line in f:\n",
    "    val = line.split()\n",
    "    word = val[0]\n",
    "    coff = np.asarray(val[1:],dtype = 'float')\n",
    "    embedd_index[word] = coff\n",
    "    \n",
    "print(len(embedd_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10609\n",
      "<class 'dict'>\n",
      "(10610, 100)\n"
     ]
    }
   ],
   "source": [
    "index_of_words = tokenizer.word_index\n",
    "print(len(index_of_words))\n",
    "embed_dims = 100\n",
    "max_len = 1000\n",
    "embedding_matrix = np.zeros((len(index_of_words)+1, embed_dims))\n",
    "\n",
    "for word,i in index_of_words.items():\n",
    "    temp = embedd_index.get(word)\n",
    "    if temp is not None:\n",
    "        embedding_matrix[i] = temp\n",
    "print(type(index_of_words))\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = embedding_matrix.shape[0]\n",
    "max_words = max_len\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 48, 100)           1061000   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                42240     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 1,103,565\n",
      "Trainable params: 1,103,565\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 100, input_length=X_train.shape[1],weights=[embedding_matrix],trainable=True))\n",
    "model.add(LSTM(64,return_sequences=False))\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 124848 samples, validate on 31212 samples\n",
      "Epoch 1/4\n",
      "124848/124848 [==============================] - 128s 1ms/step - loss: 0.9850 - acc: 0.6001 - val_loss: 0.8542 - val_acc: 0.6448\n",
      "Epoch 2/4\n",
      "124848/124848 [==============================] - 122s 974us/step - loss: 0.7984 - acc: 0.6694 - val_loss: 0.8108 - val_acc: 0.6630\n",
      "Epoch 3/4\n",
      "124848/124848 [==============================] - 125s 1000us/step - loss: 0.7368 - acc: 0.6920 - val_loss: 0.8000 - val_acc: 0.6671\n",
      "Epoch 4/4\n",
      "124848/124848 [==============================] - 120s 960us/step - loss: 0.6945 - acc: 0.7086 - val_loss: 0.7940 - val_acc: 0.6737\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x227170546d8>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_val, y_val),epochs=4, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(rev):\n",
    "    rev = re.sub('[^a-zA-Z]',' ',rev) \n",
    "    rev = rev.lower()\n",
    "    rev = rev.split()\n",
    "    rev = [ps.stem(word) for word in rev]\n",
    "    rev = ' '.join(rev)\n",
    "    rev = tokenizer.texts_to_sequences(rev)\n",
    "    rev = sequence.pad_sequences(rev, maxlen=48)\n",
    "    return(model.predict_classes(rev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good movie\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "inp = input()\n",
    "print(pred(inp)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
